{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "import sys\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "res = open(\"wisdm_main_ver_0.0/main_result/result.txt\", 'w+')\n",
    "sys.stdout = res\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"Loading Data Set...\")\n",
    "# Data Ingestion and Extraction\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').                 \\\n",
    "                                options(header='true', inferschema='true').\\\n",
    "                                load('wisdm_main_ver_0.0/data/wisdm_data.csv')\n",
    "\n",
    "drop_list = ['USER','X0','X1','X2','X3','X4','X5','X6','X7','X8','X9',\\\n",
    "            'Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9',        \\\n",
    "            'Z0','Z1','Z2','Z3','Z4','Z5','Z6','Z7','Z8','Z9']\n",
    "\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "print(\"Data Schema------------------------------------------------------------\")\n",
    "data.printSchema()\n",
    "print(\"Sample Data------------------------------------------------------------\")\n",
    "data.show(5)\n",
    "\n",
    "# SQL Injection\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Activity Count----------------------------------------------------------\")\n",
    "data.groupBy(\"activity\")          \\\n",
    "    .count()                      \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "pd.DataFrame(data.take(10), columns=data.columns).transpose()\n",
    "numeric_features = [t[0] for t in data.dtypes if (t[1] == 'double' or t[1]=='int')]\n",
    "print(\"Summary---------------------------------------------------------------\")\n",
    "print(data.select(numeric_features).describe().toPandas().transpose())\n",
    "\n",
    "# Creating Dataframe\n",
    "cols = data.columns\n",
    "df = data.select(cols)\n",
    "\n",
    "print(\"\\n===========================MODELING PIPELINE==============================\\n\")\n",
    "# Model Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['XPEAK','YPEAK','ZPEAK']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'ACTIVITY', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['XAVG','YAVG','ZAVG','XABSDEV','YABSDEV','ZABSDEV','XSTDDEV','YSTDDEV','ZSTDDEV','RESULTANT']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "print(\"Model Pipeline Schema------------------------------------------------------------\")\n",
    "df.printSchema()\n",
    "print(\"Sample Feature Data------------------------------------------------------------\")\n",
    "print(pd.DataFrame(df.take(5), columns=df.columns))\n",
    "\n",
    "# Partition Training & Test sets\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "#train.toPandas().to_csv('wisdm_main_ver_0.0/train/train.csv')\n",
    "#test.toPandas().to_csv('wisdm_main_ver_0.0/test/test.csv')\n",
    "print(\"\\n===========================TRAINING AND TESTING==============================\\n\")\n",
    "print(\"Training Dataset Count : \" + str(train.count()))\n",
    "print(\"Test Dataset Count     : \" + str(test.count()))\n",
    "\n",
    "minimized_view = ['XPEAK','YPEAK','ZPEAK','XABSDEV','YABSDEV','ZABSDEV']\n",
    "\n",
    "train.select([column for column in test.columns if column not in minimized_view]).show(5)\n",
    "test.select([column for column in test.columns if column not in minimized_view]).show(5)\n",
    "\n",
    "# Modifying Test \n",
    "skipped    = ['XPEAK','YPEAK','ZPEAK',      \\\n",
    "              'XAVG','YAVG','ZAVG',         \\\n",
    "              'XABSDEV','YABSDEV','ZABSDEV',\\\n",
    "              'XSTDDEV','YSTDDEV','ZSTDDEV',\\\n",
    "              'RESULTANT','ACTIVITY']\n",
    "test_data  = test.select([column for column in test.columns if column not in skipped])\n",
    "#test_data.toPandas().to_csv(r'wisdm_main_ver_0.0/test/test_data_minimal.csv')\n",
    "test_data.show(5)\n",
    "\n",
    "print(\"============================CLASSIFICATION AND EVALUATION============================\")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#******************************************************************\n",
    "# Logistic Regression\n",
    "#******************************************************************\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "t0 = time()\n",
    "lrModel = lr.fit(train)\n",
    "lRtt = round((time()-t0),3)\n",
    "print(lrModel)\n",
    "print (\"Classifier trained in %g seconds\"%lRtt)\n",
    "t0 = time()\n",
    "predictions = lrModel.transform(test_data)\n",
    "lRst = round((time()-t0),3)\n",
    "print (\"Prediction made in %g seconds\"%lRst)\n",
    "\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 5) \\\n",
    "    .select(\"UID\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)\n",
    "\n",
    "# Binary Classification Evaluator\n",
    "print(\"\\n-----------Binary Classification Evaluator-------------\\n\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\")\n",
    "lRRaw = evaluator.evaluate(predictions)\n",
    "print(\"Binary Classifier Raw Prediction ------------: %g\"%lRRaw)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderPR\")\n",
    "lRAuPR = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under PR --------------: %g\"%lRAuPR)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderROC\")\n",
    "lRAuROC = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under ROC -------------: %g\"%lRAuROC)\n",
    "\n",
    "# MultiClass Classification Evaluator\n",
    "print(\"\\n-----------MultiClass Classification Evaluaton---------\\n\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\")\n",
    "lRf1=evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(\"MultiClass F1 -------------------------------: %g\"%lRf1)\n",
    "lRwP=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"MultiClass Weighted Precision ---------------: %g\"%lRwP)\n",
    "lRwR=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "print(\"MultiClass Weighted Recall ------------------: %g\"%lRwR)\n",
    "lRaccuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(\"MultiClass Accuracy -------------------------: %g\"%lRaccuracy)\n",
    "\n",
    "# Regression Evaluator\n",
    "print(\"\\n----------------Regression Evaluator-------------------\\n\")\n",
    "\n",
    "#metric name in evaluation - one of:\n",
    "#rmse - root mean squared error (default)\n",
    "#mse - mean squared error\n",
    "#r2 - r^2 metric\n",
    "#mae - mean absolute error.\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lRrmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data -: %g\" % lRrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "lRmse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error on test data -------------: %g\" % lRrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "lRr2 = evaluator.evaluate(predictions)\n",
    "print(\"R^2 metric on test data ---------------------: %g\" % lRr2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "lRmae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error on test data ------------: %g\" % lRmae)\n",
    "\n",
    "# Additional Param\n",
    "print(\"\\n------------------Additional Factors--------------------\\n\")\n",
    "lp = predictions.select( \"label\", \"prediction\")\n",
    "lRcountTotal = predictions.count()\n",
    "\n",
    "print(\"Total Count          = %g\"%lRcountTotal)\n",
    "\n",
    "lRcorrect=lp.filter(col('label')== col('prediction')).count()\n",
    "print(\"Total Correct        = %g\"%lRcorrect)\n",
    "lRwrong = lp.filter(~(col('label') == col('prediction'))).count()\n",
    "print(\"Total Wrong          = %g\"%lRwrong)\n",
    "\n",
    "lRratioWrong=float(float(lRwrong)/float(lRcountTotal))\n",
    "print(\"Wrong Ratio          = %g\"%lRratioWrong)\n",
    "lRratioCorrect=float(float(lRcorrect)/float(lRcountTotal))\n",
    "print(\"Right Ratio          = %g\"%lRratioCorrect)\n",
    "print(\"\\n*********************************************************\\n\")\n",
    "\n",
    "# Cross Validator \n",
    "#***************************************************************************\n",
    "# Create 5-fold CrossValidator for Logistic Regression\n",
    "#***************************************************************************\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "t0 = time()\n",
    "cvModel = cv.fit(train)\n",
    "lRcFtt = round((time()-t0),3)\n",
    "print(str(cvModel)+\" for Logistic Regression\")\n",
    "print (\"Classifier trained in %g seconds\"%lRcFtt)\n",
    "t0 = time()\n",
    "predictions = cvModel.transform(test_data)\n",
    "lRcFst = round((time()-t0),3)\n",
    "print (\"Prediction made in %g seconds\"%lRcFst)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"UID\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)\n",
    "\n",
    "# Binary Classification Evaluator\n",
    "print(\"\\n-----------Binary Classification Evaluator-------------\\n\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\")\n",
    "lRcFRaw = evaluator.evaluate(predictions)\n",
    "print(\"Binary Classifier Raw Prediction ------------: %g\"%lRcFRaw)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderPR\")\n",
    "lRcFAuPR = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under PR --------------: %g\"%lRcFAuPR)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderROC\")\n",
    "lRcFAuROC = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under ROC -------------: %g\"%lRcFAuROC)\n",
    "\n",
    "# MultiClass Classification Evaluator\n",
    "print(\"\\n-----------MultiClass Classification Evaluaton---------\\n\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\")\n",
    "lRcFf1=evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(\"MultiClass F1 -------------------------------: %g\"%lRcFf1)\n",
    "lRcFwP=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"MultiClass Weighted Precision ---------------: %g\"%lRcFwP)\n",
    "lRcFwR=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "print(\"MultiClass Weighted Recall ------------------: %g\"%lRcFwR)\n",
    "lRcFaccuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(\"MultiClass Accuracy -------------------------: %g\"%lRcFaccuracy)\n",
    "\n",
    "# Regression Evaluator\n",
    "print(\"\\n----------------Regression Evaluator-------------------\\n\")\n",
    "\n",
    "#metric name in evaluation - one of:\n",
    "#rmse - root mean squared error (default)\n",
    "#mse - mean squared error\n",
    "#r2 - r^2 metric\n",
    "#mae - mean absolute error.\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lRcFrmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data -: %g\" % lRcFrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "lRcFmse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error on test data -------------: %g\" % lRcFrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "lRcFr2 = evaluator.evaluate(predictions)\n",
    "print(\"R^2 metric on test data ---------------------: %g\" % lRcFr2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "lRcFmae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error on test data ------------: %g\" % lRcFmae)\n",
    "\n",
    "# Additional Param\n",
    "print(\"\\n------------------Additional Factors--------------------\\n\")\n",
    "lp = predictions.select( \"label\", \"prediction\")\n",
    "lRcFcountTotal = predictions.count()\n",
    "\n",
    "print(\"Total Count          = %g\"%lRcFcountTotal)\n",
    "\n",
    "lRcFcorrect=lp.filter(col('label')== col('prediction')).count()\n",
    "print(\"Total Correct        = %g\"%lRcFcorrect)\n",
    "lRcFwrong = lp.filter(~(col('label') == col('prediction'))).count()\n",
    "print(\"Total Wrong          = %g\"%lRcFwrong)\n",
    "\n",
    "lRcFratioWrong=float(float(lRcFwrong)/float(lRcFcountTotal))\n",
    "print(\"Wrong Ratio          = %g\"%lRcFratioWrong)\n",
    "lRcFratioCorrect=float(float(lRcFcorrect)/float(lRcFcountTotal))\n",
    "print(\"Right Ratio          = %g\"%lRcFratioCorrect)\n",
    "print(\"\\n*********************************************************\\n\")\n",
    "\n",
    "#*****************************************************************\n",
    "# Decision Tree Classifier\n",
    "#*****************************************************************\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "\n",
    "t0 = time()\n",
    "dtModel = dt.fit(train)\n",
    "dTtt = round((time()-t0),3)\n",
    "print(dtModel)\n",
    "print (\"Classifier trained in %g seconds\"%dTtt)\n",
    "t0 = time()\n",
    "predictions = dtModel.transform(test_data)\n",
    "dTst = round((time()-t0),3)\n",
    "print (\"Prediction made in %g seconds\"%dTst)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"UID\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)\n",
    "\n",
    "# Binary Classification Evaluator\n",
    "print(\"\\n-----------Binary Classification Evaluator-------------\\n\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\")\n",
    "dTRaw = evaluator.evaluate(predictions)\n",
    "print(\"Binary Classifier Raw Prediction ------------: %g\"%dTRaw)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderPR\")\n",
    "dTAuPR = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under PR --------------: %g\"%dTAuPR)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderROC\")\n",
    "dTAuROC = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under ROC -------------: %g\"%dTAuROC)\n",
    "\n",
    "# MultiClass Classification Evaluator\n",
    "print(\"\\n-----------MultiClass Classification Evaluaton---------\\n\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\")\n",
    "dTf1=evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(\"MultiClass F1 -------------------------------: %g\"%dTf1)\n",
    "dTwP=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"MultiClass Weighted Precision ---------------: %g\"%dTwP)\n",
    "dTwR=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "print(\"MultiClass Weighted Recall ------------------: %g\"%dTwR)\n",
    "dTaccuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(\"MultiClass Accuracy -------------------------: %g\"%dTaccuracy)\n",
    "\n",
    "# Regression Evaluator\n",
    "print(\"\\n----------------Regression Evaluator-------------------\\n\")\n",
    "\n",
    "#metric name in evaluation - one of:\n",
    "#rmse - root mean squared error (default)\n",
    "#mse - mean squared error\n",
    "#r2 - r^2 metric\n",
    "#mae - mean absolute error.\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "dTrmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data -: %g\" % dTrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "dTmse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error on test data -------------: %g\" % dTrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "dTr2 = evaluator.evaluate(predictions)\n",
    "print(\"R^2 metric on test data ---------------------: %g\" % dTr2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "dTmae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error on test data ------------: %g\" % dTmae)\n",
    "\n",
    "# Additional Param\n",
    "print(\"\\n------------------Additional Factors--------------------\\n\")\n",
    "lp = predictions.select( \"label\", \"prediction\")\n",
    "dTcountTotal = predictions.count()\n",
    "\n",
    "print(\"Total Count          = %g\"%dTcountTotal)\n",
    "\n",
    "dTcorrect=lp.filter(col('label')== col('prediction')).count()\n",
    "print(\"Total Correct        = %g\"%dTcorrect)\n",
    "dTwrong = lp.filter(~(col('label') == col('prediction'))).count()\n",
    "print(\"Total Wrong          = %g\"%dTwrong)\n",
    "\n",
    "dTratioWrong=float(float(dTwrong)/float(dTcountTotal))\n",
    "print(\"Wrong Ratio          = %g\"%dTratioWrong)\n",
    "dTratioCorrect=float(float(dTcorrect)/float(dTcountTotal))\n",
    "print(\"Right Ratio          = %g\"%dTratioCorrect)\n",
    "print(\"\\n*********************************************************\\n\")\n",
    "\n",
    "#*****************************************************************\n",
    "# Random Forest Classifier\n",
    "#*****************************************************************\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label',numTrees = 100,maxDepth = 4,maxBins = 32)\n",
    "\n",
    "t0 = time()\n",
    "rfModel = rf.fit(train)\n",
    "rFtt = round((time()-t0),3)\n",
    "print(rfModel)\n",
    "print (\"Classifier trained in %g seconds\"%rFtt)\n",
    "t0 = time()\n",
    "predictions = rfModel.transform(test_data)\n",
    "rFst = round((time()-t0),3)\n",
    "print (\"Prediction made in %g seconds\"%rFst)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"UID\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)\n",
    "\n",
    "# Binary Classification Evaluator\n",
    "print(\"\\n-----------Binary Classification Evaluator-------------\\n\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\")\n",
    "rFRaw = evaluator.evaluate(predictions)\n",
    "print(\"Binary Classifier Raw Prediction ------------: %g\"%rFRaw)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderPR\")\n",
    "rFAuPR = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under PR --------------: %g\"%rFAuPR)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\",metricName=\"areaUnderROC\")\n",
    "rFAuROC = evaluator.evaluate(predictions)\n",
    "print(\"Binary Clasifier Area Under ROC -------------: %g\"%rFAuROC)\n",
    "\n",
    "# MultiClass Classification Evaluator\n",
    "print(\"\\n-----------MultiClass Classification Evaluaton---------\\n\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\")\n",
    "rFf1=evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(\"MultiClass F1 -------------------------------: %g\"%rFf1)\n",
    "rFwP=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"MultiClass Weighted Precision ---------------: %g\"%rFwP)\n",
    "rFwR=evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "print(\"MultiClass Weighted Recall ------------------: %g\"%rFwR)\n",
    "rFaccuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(\"MultiClass Accuracy -------------------------: %g\"%rFaccuracy)\n",
    "\n",
    "# Regression Evaluator\n",
    "print(\"\\n----------------Regression Evaluator-------------------\\n\")\n",
    "\n",
    "#metric name in evaluation - one of:\n",
    "#rmse - root mean squared error (default)\n",
    "#mse - mean squared error\n",
    "#r2 - r^2 metric\n",
    "#mae - mean absolute error.\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rFrmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data -: %g\" % rFrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "rFmse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error on test data -------------: %g\" % rFrmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rFr2 = evaluator.evaluate(predictions)\n",
    "print(\"R^2 metric on test data ---------------------: %g\" % rFr2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rFmae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error on test data ------------: %g\" % rFmae)\n",
    "\n",
    "# Additional Param\n",
    "print(\"\\n------------------Additional Factors--------------------\\n\")\n",
    "lp = predictions.select( \"label\", \"prediction\")\n",
    "rFcountTotal = predictions.count()\n",
    "\n",
    "print(\"Total Count          = %g\"%rFcountTotal)\n",
    "\n",
    "rFcorrect=lp.filter(col('label')== col('prediction')).count()\n",
    "print(\"Total Correct        = %g\"%rFcorrect)\n",
    "rFwrong = lp.filter(~(col('label') == col('prediction'))).count()\n",
    "print(\"Total Wrong          = %g\"%rFwrong)\n",
    "\n",
    "rFratioWrong=float(float(rFwrong)/float(rFcountTotal))\n",
    "print(\"Wrong Ratio          = %g\"%rFratioWrong)\n",
    "rFratioCorrect=float(float(rFcorrect)/float(rFcountTotal))\n",
    "print(\"Right Ratio          = %g\"%rFratioCorrect)\n",
    "print(\"\\n*********************************************************\\n\")\n",
    "\n",
    "\n",
    "with open('wisdm_main_ver_0.0/main_result/additional_param.csv', mode='a') as paramFile:\n",
    "    fieldnames = ['Classifier','Count Total','Correct','Wrong','Ratio Wrong','Ratio Correct','F1 Score','Training Time','Testing Time','Accuracy']\n",
    "    writer = csv.DictWriter(paramFile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'Classifier':lrModel,'Count Total':lRcountTotal,'Correct':lRcorrect,'Wrong':lRwrong,\\\n",
    "                     'Ratio Wrong':lRratioWrong,'Ratio Correct':lRratioCorrect,'F1 Score':lRf1,\\\n",
    "                     'Training Time':lRtt,'Testing Time':lRst,'Accuracy':lRaccuracy})\n",
    "    writer.writerow({'Classifier':dtModel,'Count Total':dTcountTotal,'Correct':dTcorrect,'Wrong':dTwrong,\\\n",
    "                     'Ratio Wrong':dTratioWrong,'Ratio Correct':dTratioCorrect,'F1 Score':dTf1,\\\n",
    "                     'Training Time':lRtt,'Testing Time':lRst,'Accuracy':dTaccuracy})\n",
    "    writer.writerow({'Classifier':rfModel,'Count Total':rFcountTotal,'Correct':rFcorrect,'Wrong':rFwrong,\\\n",
    "                     'Ratio Wrong':rFratioWrong,'Ratio Correct':rFratioCorrect,'F1 Score':rFf1,\\\n",
    "                     'Training Time':lRtt,'Testing Time':lRst,'Accuracy':rFaccuracy})\n",
    "\n",
    "with open('wisdm_main_ver_0.0/main_result/crossFold_additional_param.csv', mode='a') as cVFile:\n",
    "    fieldnames = ['Classifier','Count Total','Correct','Wrong','Ratio Wrong','Ratio Correct','F1 Score','Cross Validation Training Time','Cross Validation Testing Time','Cross Fold Accuracy']\n",
    "    writer = csv.DictWriter(cVFile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'Classifier':lrModel,'Count Total':lRcFcountTotal,'Correct':lRcFcorrect,'Wrong':lRcFwrong,\\\n",
    "                     'Ratio Wrong':lRcFratioWrong,'Ratio Correct':lRcFratioCorrect,'F1 Score':lRcFf1,\\\n",
    "                     'Cross Validation Training Time':lRcFtt,'Cross Validation Testing Time':lRcFst,'Cross Fold Accuracy':lRcFaccuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
